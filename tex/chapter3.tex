\chapter{PHYSICS OBJECT RECONSTRUCTION} \label{reco}

In the early days of particle physics experimentation, charged particles were visually identified by analyzing photographs of the ionization tracks left behind in cloud chambers and bubble chambers. Given the higher collision energies and instantaneous luminosities demanded by modern experiments, the amount of information recorded for a collision event renders such visual analyses intractable. The final state particles produced by a proton collision at the LHC are recorded as electronic signals by the CMS detector, and the accurate interpretation of these signals as physics objects is what enables the full reconstruction of the collision's aftermath. The definition and reconstruction of the standard physics objects, with an emphasis on those used by the \VHbb\ analysis, are described in this chapter.

\section{Particle Flow Algorithm}

The hermetic design of the CMS detector and the granularity of its subsystems enabled the first successful deployment of a \textit{particle flow (PF)} based reconstruction algorithm at a hadron collider experiment.\cite{PARTICLEFLOW} Although the individual subsystems are capable of reconstructing the particles for which they were designed, a more accurate and global event description can be achieved by combining the measurements obtained by the subsystems as a whole. Since its commissioning, the PF algorithm has been used online to improve the efficiency of the High-Level Trigger (HLT) and offline to improve the quality of the reconstructed particle candidates considered by physics analyses.

The PF algorithm starts by collecting the detector-level objects, or \textit{elements}, produced by each detector subsystem: the silicon tracker and muon chambers both provide charged particle \textit{tracks}, while the electromagnetic calorimeter (ECAL) and hadronic calorimeter (HCAL) both provide \textit{clusters} of absorbed energy. A \textit{link algorithm} which tests the compatibility of pairs of elements from different subsystems is used to generate \textit{blocks} of elements that are directly linked or indirectly linked through common elements. Individual particles are subsequently identified and reconstructed within each block, starting with muons then proceeding to electrons, photons, and charged and neutral hadrons. As particles are reconstructed, the elements associated with that particle are removed from the block such that each particle is reconstructed from a set of unique elements. Once all the blocks have been processed and all particles in the event have been identified and reconstructed, a post-processing algorithm is used to correct misidentified or misreconstructed muons with large transverse momentum \pT\ which can artificially increase the reconstructed missing transverse momentum \pTmiss\ in the event.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=5in]{images/pfdiagram}
    \caption[Particle Flow Infographic]{An infographic illustrating the particle flow reconstruction paradigm.\cite{pfdiagram}}
    \label{fig:pfdiagram}
\end{figure}

At this stage, the particle candidates proposed by the PF algorithm are ready to be used in physics analyses. In practice, the particle candidates are processed further by passing them to algorithms which employ different clustering strategies to reconstruct jets. Finally, The candidate particles and jets which satisfy the additional criteria recommended by the various physics object groups (POGs) within the CMS collaboration become the standard physics objects considered by the physics analyses.

\subsection{Detector-level Objects}

Each detector subsystem produces objects which become the fundamental inputs to the PF algorithm. The two main types of detector-level objects are charged particle tracks, with a distinction between tracks from the silicon tracker and standalone muon tracks from the muon chambers, and calorimeter clusters. By considering these objects together, the full information gathered by the detector may be brought to bear for the task of identifying and reconstructing particles.

\subsubsection{Tracks}

The silicon tracker, the inner-most detector subsystem of the CMS detector, reconstructs tracks from \textit{hits}, which are clusters of signals gathered by the pixel and strip trackers. This task quickly becomes a combinatorial challenge because of its dense operating environment due to \textit{pileup}. The bunched nature of the LHC's proton beams results in multiple proton-proton collisions each time they cross the interaction point as illustrated in Figure \ref{fig:pileup}, and those collisions which overlap the one of interest are deemed pileup interactions. Although the number of hits in the tracker increases linearly with pileup, the number of possible combinations of hits to form tracks grows exponentially.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=4in]{images/pileup}
    \caption[Event Display of High-Pileup Collision Event]{An event display for a collision from the high-pileup run 198609 which shows 78 reconstructed vertices and their associated charged particle tracks.\cite{pileup}}
    \label{fig:pileup}
\end{figure}

In order to accurately and quickly reconstruct tracks, an iterative tracking algorithm\cite{ITERTRACK} is employed by the CMS experiment which applies several iterations of a combinatorial track finder (CTF) based on Kalman filtering\cite{CMSTRACKRECO} to a collection of hits. Each iteration of the CTF proceeds through the following stages:

\begin{itemize}
  \item \textbf{Seed Generation:} Initial trajectory candidates composed of three hits or two hits and a beamspot or vertex constraint are proposed as seeds.
  \item \textbf{Trajectory Building:} The seeds are extrapolated towards compatible hits in subsequent layers and also towards a single ``invalid'' or fake hit to account for the case where the corresponding hit was not recorded. A Kalman filter then updates the trajectory based on the compatible hit to form track candidates. This extrapolation procedure terminates upon reaching the outermost layer of the tracker or a when ``stop condition'' is satisfied, such as surpassing a maximum number of invalid hits. Because a single seed may produce multiple track candidates and different seeds may produce the same track candidate, an ambiguity resolution based on the fraction of hits shared between pairs of track candidates is applied to prevent double counting.
  \item \textbf{Track Fitting:} The collection of hits for each track candidate is refitted by a Kalman filter and smoother to obtain optimal estimates of the track parameters illustrated in Figure \ref{fig:track_params}: the signed transverse curvature $q\pT$, the polar angle $\cot \theta_{0}$, the azimuthal angle $\phi_{0}$, the longitudinal impact parameter $d_{z}$, and the signed transverse impact parameter $d_{0}$, all of which are defined at the point of closest approach to the beam axis.
\end{itemize}

\begin{figure}[htbp]
  \centering
    \includegraphics[width=3.5in]{images/track_parameters}
    \caption[Charged Particle Trajectory Parameterization]{The parameterization of a charged particle's trajectory through a magnetic field.\cite{salzburger}}
    \label{fig:track_params}
\end{figure}

The CTF is run for a total of ten iterations. The first iteration seeks to reconstruct the easiest tracks which originate close to the interaction point, also called \textit{prompt} tracks, and have high \pT. The last two iterations attempt to reconstruct muons and involves muon tracks which are described in further detail in section \ref{standalonemuontrack}. Because successive iterations face increasingly difficult tasks, the hits associated with selected track candidates are masked in subsequent iterations to reduce the combinatorial complexity. The seeds and reconstruction targets of the ten iterations are summarized by Table \ref{tbl:itertrack}.

\begin{table}[htbp]
  \caption[Iterative Tracking Summary]{A summary of the ten tracking iterations used by the CMS tracker. The variable $R$ in the final column denotes the distance from the beam axis to the production position of the targeted track.\cite{PARTICLEFLOW}}
  \label{tbl:itertrack}
  \small
  \begin{tabularx}{6.5in}{XXl}
    \hline
    Iteration & Seed Type            & Reconstructed Tracks                        \\
    \hline
    1         & pixel triplets       & prompt, high \pT                            \\
    2         & pixel triplets       & from \qrkb-hadron decays, $R \lesssim 5$ cm \\
    3         & pixel triplets       & prompt, low \pT                             \\
    4         & pixel pairs          & recover high \pT                            \\
    5         & pixel+strip triplets & displaced, $R \lesssim 7$ cm                \\
    6         & strip triplets/pairs & very displaced, $R \lesssim 25$ cm          \\
    7         & strip triplets/pairs & very displaced, $R \lesssim 60$ cm          \\
    8         & pixel+strip pairs    & inside high \pT\ jets                       \\
    9         & muon-tagged tracks   & muons                                       \\
    10        & muon detectors       & muons                                       \\
    \hline
  \end{tabularx}
\end{table}

\begin{figure}[htbp]
  \centering
  \mbox{
    \subfigure [] {\includegraphics[scale=0.35]{images/pftrack_eff}} \qquad
    \subfigure [] {\includegraphics[scale=0.35]{images/pftrack_misreco}} \qquad
  }
  \caption[Track Reconstruction Performance]{The A) tracking efficiency and B) misreconstruction rate of charged hadron tracks in simulated multijet events for a single, global iteration of the combinatorial track finder (black squares), the prompt iterations (iterations 1, 2, 3, 4, 5, and 7) of the iterative tracking method (green triangles), and the full iterative tracking method (red circles) as a function of track \pT.\cite{PARTICLEFLOW}}
    \label{fig:pftrack_perf}
\end{figure}

The tracking efficiency and misreconstruction rate are shown in Figure \ref{fig:pftrack_perf}. The full iterative tracking method achieves a 90\% reconstruction efficiency for tracks with \pT\ between 1 \GeV\ and 10 \GeV, to be compared with the 70-80\% efficiency of the global CTF, and maintains an approximately 8-10\% higher efficiency than the global CTF for tracks with $\pT > 100\ \GeV$. Although the iterative tracking method incurs a slightly higher misreconstruction rate than the global CTF, it is able to reconstruct over half of the tracks missed by the global CTF while running twice as fast.

\subsubsection{Calorimeter clusters}

The ECAL and HCAL, which are the intermediate detector subsystems of the CMS detector, reconstruct energy clusters left behind by traversing particles. The electromagnetic interactions of particles with the ECAL materials induce electromagnetic showers which deposit their energy in lead tungstate crystals. The strong interactions of particles with the HCAL materials induce hadronic showers and jets which deposit their energies in alternating layers of brass or steel absorbing plates and plastic scintillator. The electronic signals produced by these showers are measured by the cells of their respective calorimeter.

The ECAL clusters are seeded by cells which contain a local energy maxima above a detector-dependent threshold. The \textit{Island algorithm} is used in both the ECAL barrel and endcaps to cluster the eight cells directly adjacent to the seed in the $\eta\phi$-plane with measured energies above twice the noise level, while the \textit{hybrid algorithm} is used in the ECAL barrel only and forms bars that are 3 or 5 crystals wide in $eta$ before searching along the $phi$-direction. Both of these algorithms propose \textit{topological clusters} which are themselves clustered into \textit{superclusters}. The most energetic topological cluster is used as the supercluster seed, and nearby clusters are gathered into a supercluster with some spread in the $phi$-direction to recover the energy from photon conversions and bremsstrahlung which are spread by the solenoid's magnetic field. Although 94\% of the energy of a photon or electron is contained by a $3\times3$ cluster of crystals, and up to 97\% for $5\times5$ clusters, detector materials before the ECAL result in energy losses which are accounted for by applying corrections to the supercluster energies. Clusters are similarly constructed using information from the HCAL, although the algorithms proceed by considering only those four cells which share an edge with the seed cell. A visualization of ECAL and HCAL clusters are shown in Figure \ref{fig:caloclusters}.

\begin{figure}[htbp]
  \centering
  \mbox{
    \subfigure [] {\includegraphics[scale=0.35]{images/ecalclusters}} \qquad
    \subfigure [] {\includegraphics[scale=0.35]{images/hcalclusters}} \qquad
  }
  \caption[Event Display of Calorimeter Clusters]{An event display of a simulated jet of five particles projected onto A) the ECAL surface and B) the HCAL surface. The green lines represent reconstructed tracks while the red dots represent reconstructed cluster centroids. Four ECAL clusters and two HCAL clusters are shown.\cite{PARTICLEFLOW}}
    \label{fig:caloclusters}
\end{figure}

\subsubsection{Standalone muon tracks}\label{standalonemuontrack}

The muon chambers also reconstruct tracks from hits and, being the outer-most detector subsystems of the CMS detector, it is expected that these tracks represent muon trajectoriees. For drift tubes (DTs), hits are reconstructed along anode wires based on the measured arrival time of electrons produced when a traversing muon ionizes the gas in the drift cell. For cathode strip chambers (CSCs), hits are reconstructed from the intersection of clustered signals from cathode strips and anode wire groups. For resistive plate chambers (RPCs), the hits are reconstructed as the centroid of the clustered strip signals. Because the CSCs and DTs have multiple layers, as opposed to the single layer design of the RPCs, their hits are also locally reconstructed as \textit{track segments}.

Groups of DT and CSC track segments from the innermost chambers are used as seeds to generate trajectory candidates. A Kalman filter\cite{MUONKF} is used to form track candidates by combining the seed with track segments and hits found in the outer layers. This process is also repeated in reverse, from the outer layers towards the track seed, and the resulting pair is combined into a single track candidate which is then extrapolated to the beam axis and refit with a vertex constraint. This optimal track candidate, based solely on the information obtained by the muon chambers, is what is referred to as a \textit{standalone muon track}.

\subsection{Particle Candidates}

The PF algorithm identifies and reconstructs candidate electrons, photons, muons, and charged and neutral hadrons using the elements contributed by the various detector subsystems. Although not an exhaustive list, these five basic particle types are sufficient for the further reconstruction of more complex objects such as jets or \lept\ leptons. As discussed in the following sections, the benefits of this holistic approach towards particle reconstruction are realized as more robust particle candidates and improved estimates of their properties.

\subsubsection{Electrons}

Electron candidates are formed from seed tracks and their associated ECAL clusters which are compatible with the trajectory extrapolated from the track. The emission of photons by electrons as they travel through the silicon tracker poses a difficulty for the tracking algorithm which relies on pattern recognition, so a Gaussian-sum filter (GSF)\cite{GSFTRACK} is used to recover hits for the seed track which may have been missed during the trajectory building step. The direction of the seed track is taken to be the direction of the electron candidate and a combination of the seed track momentum and corrected ECAL cluster energies are used to assign the energy of the electron candidate.

This method of electron reconstruction achieves a high identification efficiency for genuine electrons but also tends to misidentify charged hadrons as electrons. The electron identification can be improved by training boosted decision tree (BDT) classifiers to discriminate against fake electrons using fourteen electron candidate features.\cite{CMSELEPERF} Those candidates with classifier scores above an analysis-dependent threshold are identified as electrons. The electron reconstruction efficiency as measured with a data sample of proton-proton collisions collected at $\sqrt{s} = 8\ \TeV$ and corresponding to an integrated luminosity of 19.7 \invfb\ is shown in Figure \ref{fig:elerecoeff}. The reconstruction efficiency is at least 85\% for electrons with $\pT > 15\ \GeV$ and plateaus at approximately 97\% for electrons with $\pT > 30\ \GeV$.

\begin{figure}[htbp]
  \centering
  \mbox{
    \subfigure [] {\includegraphics[scale=0.4]{images/elerecoeff_loweta}} \qquad
    \subfigure [] {\includegraphics[scale=0.4]{images/elerecoeff_higheta}} \qquad
  }
  \caption[Electron Reconstruction Efficiency]{The electron reconstruction efficiency as a function of electron \pT\ for dielectron data events (black points) and Drell-Yan simulation (blue triangles) in regions of A) low $\eta$ and B) high $\eta$. The medium working point derived from the BDT classifier was used as the multivariate selection.\cite{CMSELEPERF}}
    \label{fig:elerecoeff}
\end{figure}

\subsubsection{Photons}

Photon candidates are seeded by ECAL superclusters which are not linked to a track. However, photons travelling through the silicon tracker are likely to interact with the material and convert into $e^{+}e^{-}$ pairs. To ensure that \textit{converted} photons are correctly reconstructed, pairs of oppositely-charged tracks identified as electrons are associated with the supercluster.

The identification of photons is also improved by training a BDT to discriminate prompt photons from background using features which describe the shape of the shower in the ECAL and aspects of the photon candidates supercluster.\cite{CMSPHOTPERF} Those candidates with classifier scores above an analysis-dependent threshold are identified as photons. The photon reconstruction efficiency measured with a data sample of proton-proton collisions collected at $\sqrt{s} = 8\ \TeV$ and corresponding to an integrated luminosity of 19.7 \invfb\ is shown in Figure \ref{fig:photrecoeff}. For the BDT threshold that achieves 80\% signal efficiency and approximately 93\% background rejection, the photon reconstruction efficiency typically falls between 80-90\% for photons with $\pT > 20\ \GeV$.

\begin{figure}[htbp]
  \centering
  \mbox{
    \subfigure [] {\includegraphics[scale=0.4]{images/photrecoeff_loweta}} \qquad
    \subfigure [] {\includegraphics[scale=0.4]{images/photrecoeff_higheta}} \qquad
  }
  \caption[Photon Reconstruction Efficiency]{The photon reconstruction efficiency as a function of photon \pT\ for dielectron data events (black points) and Drell-Yan simulation (blue triangles) in regions of A) low $\eta$ and B) high $\eta$. An example working point for the BDT classifier which achives 80\% signal efficiency was used as the multivariate selection.\cite{CMSPHOTPERF}}
    \label{fig:photrecoeff}
\end{figure}

\subsubsection{Muons}

Muon candidates are seeded by standalone muon tracks, as well as \textit{tracker muon tracks} and \textit{global muon tracks} which are formed by combining information from the silicon tracker and muon chambers during the last two iterations of the iterative tracking algorithm. The tracker muon tracks are generated by an ``inside-out'' algorithm which extrapolates tracks from the silicon tracker which pass a momentum threshold to track segments identified in the DT and CSC chambers which are compatible with the hypothesized trajectory. Global muon tracks are similarly generated using an ``outside-in'' algorithm which extrapolates standalone muon tracks to tracks from the silicon tracker, followed by a combined Kalman filter fit. Those tracker muon tracks and global muon tracks which share silicon tracker tracks are identified as the same muon candidate. If any calorimeter clusters are associated with the muon tracks, they must be compatible with the energy deposition signature of a minimum ionizing particle.

A variety of muon identification algorithms are defined by the CMS Muon POG to serve the needs of different physics analyses. These algorithms combine low-level variables, such as the number of hits per track, and high-level variables, such as the compatibility between the tracks, track segments, and primary vertex, to identify specific types of muons under different conditions. For example, the common \textit{loose muon ID} is designed to efficiently identify tracker muons or global muons as prompt muons which originate from the primary vertex or light and heavy flavor decays.\cite{CMSMUONPERF} The reconstruction efficiency when using the loose muon ID, which exceeds 99\% over the full $\eta$ range, is measured using a 2015 data sample of proton-proton collisions collected at $\sqrt{s} = 13\ \TeV$ that corresponds to an integrated luminosity of approximately 2 \invfb\ and is shown in Figure \ref{fig:muonrecoperf}.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=3in]{images/muonrecoeff}
    \caption[Muon Reconstruction Efficiency]{The muon reconstruction efficiency as a function of $\eta$ for 2015 data events with a pair of reconstructed muons (black points) and Drell-Yan simulation (blue squares). The muon candidates were required to have $\pT > 20\ \GeV$ and pass the loose muon ID.\cite{CMSMUONPERF}}
    \label{fig:muonrecoperf}
\end{figure}

\subsubsection{Charged and neutral hadrons}

Charged hadron candidates are seeded by HCAL clusters which are linked to at least one track and possibly ECAL clusters. Neutral hadron candidates within the tracker's acceptance, or $\left| \eta \right| < 2.5$, are instead seeded by HCAL clusters which are not linked to a track while candidates utside of the tracker's acceptance are seeded by linked ECAL and HCAL clusters which present an energy excesses inconsistent with the signature of a charged hadron's energy deposition. Because such hadrons are created by the hadronization of final state particles, their performance is better discussed in the context of jet reconstruction and is reserved for section \ref{jetreco}.

\section{Primary Vertices}

Although multiple proton-proton interactions occur when the beams collide, only those interactions which cause the event to trigger are of interest. The position where such an interaction occurs is known as a \textit{primary vertex} and its reconstruction is vital because it the point of origin of the particles initially produced by the hard-scattering process.

The reconstruction of primary vertices begins with the clustering of tracks using a deterministic annealing algorithm.\cite{ITERTRACK} The tracks of each primary vertex candidate are then passed to an adaptive vertex fit to improve the estimate of the spatial position of the vertex.\cite{VERTEXFIT} The fit also assigns weights to each track in the cluster based on their compatibility with the primary vertex position, the sum of which is proportional to the number of degrees of freedom of the fit. Based on their fit results, only those candidates which have a $z$ position within 24 cm of the nominal detector center, a radial distance within 2 cm of the beamspot axis, and a vertex fit exceeding four degrees of freedom are considered primary vertices.

\section{Tau Leptons}

Besides the electron and muon candidates proposed by the PF algorithm, a third lepton remains to be reconstructed: the tau lepton. With a mass of $m_{\lept} \approx 1.78\ \GeV$\cite{PDG2018}, the tau is the only lepton able to decay hadronically through weak interactions, which accounts for almost two-thirds of its decays. The remaining decay channels are purely leptonic, with the tau decaying into either an electron of muon. The hadrons-plus-strips (HPS) algorithm\cite{TAURECO1,TAURECO2} is used to reconstruct the hadronically decaying taus by analyzing the constituents within reconstructed jets. The photons and electrons within the seed jets are clustered together into \textit{strips} because neutral pions, being the most common hadron produced by tau decays, predominantly decay into photon pairs. Based on the number of strips and charged particles found within the jet, a tau candidate is assigned to one of three possible decay topologies:
\begin{itemize}
  \item \textbf{One prong:} a single charged hadron and no strips.
  \item \textbf{Two prong:} a single charged hadron and one strip.
  \item \textbf{Three prong:} a single charged hadron and two strips, or three charged hadrons.
\end{itemize}
Finally, to reduce the misidentification rate of jets as taus, a BDT is trained to classify taus using jet features, such as the impact parameter of the leading track and the flight distance, as well as jet constituent features, such as the multiplicity of electrons and photons within the jet.

\section{Lepton Isolation}

Electrons and muons can either be prompt, produced directly by electroweak decays of massive particles suchs as the \bosW\ or \bosZ\ bosons, or non-prompt, produced by the subsequent decays of taus or jets. Lepton \textit{isolation}, a measure of the lack of activity around the charged particle track, is used to reconstruct prompt electrons and muons while rejecting leptons which are jet constituents. The isolation is defined as the ratio of the total transverse momentum of the particles within a cone around the lepton to the transverse momentum of the lepton itself
\begin{equation}
  I_{\mathrm{PF}} = \frac{1}{\pT} \left( \sum_{\bosg}\pT^{\bosg} + \sum_{h^{\pm}}\pT^{h^{\pm}} + \sum_{h^{0}}\pT^{h^{0}} \right),
  \label{eq:iso}
\end{equation}
where the superscripts indicate photons (\bosg), charged hadrons ($h^{\pm}$), and neutral hadrons ($h^{0}$) and the lack of a superscript denotes the lepton of interest. The identification of prompt leptons proceeds by placing thresholds directly on the lepton isolation, with lower thresholds indicating a tighter criterion, or passing the lepton isolation as one of several inputs to a classifier.

\section{Jets}\label{jetreco}

The experimental signature of quarks and gluons produced by a hard scattering process are collimated sprays of particles known as \textit{jets}. Due to color confinement, quarks and gluons are not observed directly but through the hadrons they immediately form, which are themselves unstable and inferred from the decay of their partons. This process of hadronization and fragmentation continues until the hadrons are absorbed or the remaining decay products are stable particles. Because jet constituents leave signatures all throughout the detector volume, the holistic approach of the PF algorithm is well-suited for jet reconstruction.

\subsection{Reconstruction}

The primary concerns for jet reconstruction are the identification and association of a jet's constituent particles. The identification of possible constituent particles is undertaken by the PF algorithm, where prompt and isolated electrons, muons, and photons are removed from consideration. The assocation of constituents to form a jet is handled by specialized jet finding algorithms.

Jet finding algorithms broadly fall under two categories, cone algorithms and sequential recombination algorithms. Cone algorithms use a ``top-down'' approach based on the assumption that jets can be contained within conical envelopea. Cones of radius $R$ are first seeded along regions of energy flow and stable cones, which have axes compatible with the direction of the vectorial sum of the momenta of particles within the cone, are kept. Overlapping stable cones are further split or merged under the algorithm converges. Sequential recombination algorithms instead use a ``bottom-up'' approach based on the assumption that jet constituents form well-defined clusters due to their collimation. Distance metrics in coordinate space and in transverse momentum space are defined and calculated between pairs of constituent candidates, with the minimum distance pair of either space used to seed the jet cluster. The distances are then recalculated and candidate particles combined with the jet cluster iteratively until reaching a stop condition.

Because the theoretically motivated observable quantities describing parton decays are manifested experimentally as the properties of reconstructed jets, the implementation of jet finding algorithms must be infrared and collinear (IRC) safe. Jet finding algorithms are IRC unsafe if they are sensitive to soft emissions, such as gluon radiation, and collinear splittings of particles as illustrated in Figure \ref{fig:IRCunsafe}. By satisying the requirement of IRC safety, the jets reconstructed by the algorithm may be considered valid approximations of the final state partons.

\begin{figure}[htbp]
  \centering
  \mbox{
    \subfigure [] {\includegraphics[width=3in]{images/infrared_emission}} \qquad
    \subfigure [] {\includegraphics[width=3in]{images/collinear_emission}} \qquad
  }
  \caption[Infrared and Collinear Unsafety]{An illustration of the consequences of infrared and collinear unsafety for jet finding: A) an example of infrared unsafety, where a soft emission (orange) causes the algorithm to merge two overlapping but otherwise distinct jets; B) an example of collinear unsafety, where the collinear splitting of a contituent alters the jet returned by the algorithm.}
    \label{fig:IRCunsafe}
\end{figure}

Although cone algorithms were previously favored for their ease of implementation, simple jet geometries, and faster execution compared to sequential recombination algorithms, the vast majority of cone algorithms are not IRC safe. With the release of the \texttt{FastJet}\cite{FASTJET} software package, efficient implementations of sequential recombination algorithms with $\mathcal{O}(n \log n)$ time complexity were readily adopted at the LHC. The CMS experiment uses the \textit{anti-$k_{T}$} jet clustering algorithm\cite{ANTIKT} to reconstruct jets from PF candidates. The anti-$k_{T}$ algorithm defines the distance in transverse momentum space as
\begin{equation}
  d_{ij} = \min \left( \frac{1}{p_{Ti}^{2}}, \frac{1}{p_{Tj}^{2}} \right) \times \frac{R_{ij}^{2}}{R},
  \label{eq:antikt_dij}
\end{equation}
where $i$ and $j$ are the indices of the PF candidates, $R$ is the radius parameter which governs the size of the reconstructed jets, and $R_{ij}^{2}$ is the Euclidean distance between the PF candidates in the $\eta\phi$-plane
\begin{equation}
  R_{ij}^{2} = \left( \eta_{i} - \eta_{j} \right)^{2} + \left( \phi_{i} - \phi_{j} \right)^{2}.
  \label{eq:deltaR2}
\end{equation}
The anti-$k_{T}$ algorithm also defines an additional distance between the beam axis and the $i$-th particle to be
\begin{equation}
  d_{iB} = \frac{1}{p_{Ti}^{2}},
  \label{eq:antikt_diB}
\end{equation}
which is used to classify a finished cluster as a jet.

The algorithm proceeds by finding the minimum over the set of distances $\left\{ d_{ij}, d_{iB} \right\}$. If the minimum distance is between a pair of particles $i$ and $j$, their four-vectors are summed to form a new particle and particles $i$ and $j$ are removed from the set of particles consideredby the algorithm. If the minimum distance is between a particle and the beam axis, the particle is considered a jet and removed from the set o particles considered by the algorithm. The algorithm continues until all initial particles have been combined into jets. Because the distance measure given by Equation \ref{eq:antikt_dij} is dominated by high \pT\, the anti-$k_{T}$ algorithm tends to cluster soft particles with the hardest particles first,producing perfectly conical jets.

\subsection{Energy Scale and Resolution Corrections} \label{jesjer}

Disagreements between the measured energies of reconstructed jets and the true energies of their corresponding partons are expected. The jet's constituent particles encounter unavoidable energy losses as they travel through the detector material and their measured energies are biased by the non-linear response of the detector subsystems to their signals. Because the four-momentum of a jet is taken to be the vectorial sum of the momenta of its clustered PF candidates, these issues and more are compounded together and require a dedicated calibration.

In order to calibrate jet energies, the recommended procedure by the CMS experiment applies a series of \textit{jet energy scale} (JES) corrections which scale the four-momentum of jets. The various detector effects are addressed using a factorized approach shown in Figure \ref{fig:jeschain}, where each of the corrections are designed to target specfic effects and are applied in a fixed-order sequence. The first correction aims to remove energy contributions due to pileup particles and also spurious detector noise, and is parameterized as a function of the energy density $\rho$, jet area $A$, and the jet $\pT$ and $\eta$. The second correction addresses the effects of the calorimeter response, which is non-uniform in $\eta$ and non-linear in \pT. The next two corrections address percent level residual differences observed between the truth level simulation and data. A final, optional correction can also be applied to correct the flavor, or type of parton, which initiated the jet.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=6in]{images/jeschain}
    \caption[Jet Energy Scale Corrections Infographic]{An inforgraphic of the jet energy scale corrections used by the CMS experiment for data and Monte-Carlo simulated samples. Corrections derived from simulation are marked with MC, while RC stands for random cone and MJB stands for multijet background.\cite{CMSJES}}
    \label{fig:jeschain}
\end{figure}

After the jet energies are calibrated, the jet energy resolution (JER) in Monte-Carlo (MC) simulation is observed to be better than for data. Because the effects of detector acceptance and performance are not completely modelled in simulation, a ``smearing'' procedure is adopted which modifies the four-momenta of reconstructed jets in order to match the energy resolution observed in data. The smearing can be performed using the scaling method which scales the four-momentum with a correction of the form
\begin{equation}
  c_{\mathrm{JER}} = 1 + (s_{\mathrm{JER}} - 1) \frac{\pT - \pT^{\mathrm{ptcl}}}{\pT},
  \label{eq:smearing_scale}
\end{equation}
where $s_{\mathrm{JER}}$ is the simulation-to-data resolution scale factor, \pT\ is the transverse momentum of the reconstructed jet, and $\pT^{\mathrm{ptcl}}$ is the transverse momentum of the corresponding jet but clustered from generator-level particles. The scaling method assumes that the particle-level jet matches well the generator-level jet, otherwise it introduces a large shift in the response. The alternative method of stochastic smearing relaxes this assumption, with a randomly sampled correction of the form
\begin{equation}
  c_{\mathrm{JER}} = 1 + \mathcal{N}\left( 0, \sigma_{\mathrm{JER}} \right) \sqrt{\max \left( s_{\mathrm{JER}}^{2} - 1, 0 \right)},
  \label{eq:smearing_stochastic}
\end{equation}
where $\mathcal{N}\left( 0, \sigma_{\mathrm{JER}} \right)$ denotes a normal distribution with zero mean and variance determined by the relative \pT\ resolution in simulation $\sigma_{\mathrm{JER}}$ and $s_{\mathrm{JER}}$ is the simulation-to-data resolution scale factor. For either method, negative correction factors are set to zero. Typically, a hybird apporach is recommended where the stochastic smearing is applied by default and the scaling smearing is applied if a jet has a generator-level match.

In physics analyses, both the JES corrections and JER smearings are applied to the reconstructed jets and systematic uncertainties are assessed for the different systematic sources based on their $1\sigma$ shifts that is fully correlated in jet \pT\ and $\eta$. This amounts to at most a 3\% uncertainty in the phase space considered by most analyses. A thorough review of the derivations of the JES corrections, JER smearing, and their uncertainties and are available in Ref. \cite{CMSJES}.

\subsection{\qrkb-Tagging} \label{btagging}

The long lifetime of \qrkb-hardons, quark bound states containing a bottom quark, can be attributed to the mass of the \qrkb-quark, which is heavier than the common quarks such as the up or down quarks, and the suppression of its decays to lighter quarks by the Cabbibo-Kobayashi-Maskawa (CKM) matrix. Because of this longer lifetime, \qrkb-hadrons travel a distance on the order a few millimeters before decaying, resulting in a characteristic displaced vertex relative to the primary vertex of the hard scattering process, as shown in Figure \ref{fig:secondary_vertex}. This property enables the jets formed by the hadronization of \qrkb-quarks to be distinguished from those of other quarks by using \textit{\qrkb-tagging} methods which determine a jet's quark flavor.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=4in]{images/secondary_vertex}
    \caption[Illustration of a Heavy-Flavor Jet]{An illustration of a heavy-flavor (\qrkc\ or \qrkb-hadron) jet emphasizing its characteristic property of a secondary vertex (SV) that is displaced from the primary vertex (PV).\cite{CMSBTAG}}
    \label{fig:secondary_vertex}
\end{figure}

The latest \qrkb-tagging algorithm developed and used by the CMS experiment is the Deep Combined Secondary Vertex (DeepCSV)\cite{CMSBTAG} tagger. The DeepCSV tagger is a classifier based on a fully-connected neural network architecture with four hidden layers, each with 100 rectified linear (ReLU) units, implemented using the \texttt{Keras}\cite{KERAS} neural network framework with \texttt{Tensorflow}\cite{TENSORFLOW} as the graph compilation and automatic differentiation backend. The final layer of the neural network is composed of five linear units passed through a softmax activation function. A softmax output layer was chosen because the output values are interpreted to be the probability that a jet belongs to one of five possible classes based on its particle content: exactly one \qrkb-hadron, at least two \qrkb-hadrons, exactly one \qrkc-hadron and no \qrkb-hadrons, at least two \qrkc-hadrons and no \qrkb-hadrons, and no \qrkc\ or \qrkb-hadrons.

The DeepCSV tagger is trained using information from the secondary vertex candidates proposed by the inclusive vertex finding (IVF) algorithm, the informaton of the tracks associated with the secondary vertices such as their impact parameter, as well as the jet \pT\ and $\eta$. The combination of low-level and high-level variables enables the DeepCSV tagger to not only perform better than previous approaches in terms of \qrkb-jet identification efficency but also crucially reduces the misidentification probability of light jets as \qrkb-jets as shown in Figure \ref{fig:btagperf}. For physics analyses, three standard working points are defined for use which trade off \qrkb-jet identification effiency ($\varepsilon_{b}$) for misidentification probability ($\varepsilon_{udsg}$): the loose (L) working point achieves $\varepsilon_{b} = 84\%$ and $\varepsilon_{udsg} = 11\%$, the medium (M) working point achieves $\varepsilon_{b} = 68\%$ and $\varepsilon_{udsg} = 1.1\%$, and the tight (T) working point achieves $\varepsilon_{b} = 50\%$ and $\varepsilon_{udsg} = 0.1\%$.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=4.5in]{images/btagperf}
    \caption[CMS \qrkb-Tagging Performance]{The receiver operating characteristic (ROC)curves which characterize the performance of various \qrkb-tagging algorithms developed by the CMS experiement and evaluated using a $\qrkt\qrktbar+\mathrm{jets}$ Monte-Carlo simulated sample. The DeepCSV algorithm's \qrkb-tagging performance against light flavored jets is given by the solid purple line.\cite{CMSBTAG}}
    \label{fig:btagperf}
\end{figure}

Discrepancies are observed in the distributions of some of the input variables to the DeepCSV tagger between data and simulation which arise due to imperfect modeling of the detector and the modelling of parton showering and hadronization by the MC generators. These discrepancies translate to differences in \qrkb-tagging efficiency between data and simulation. In order to correct for these effects, a scale factor is applied on a per jet basis as a function of jet \pT and $\eta$ defined by
\begin{equation}
  \mathrm{SF}_{f} = \frac{\varepsilon_{f}^{\mathrm{data}} \left( \pT, \eta \right)}{\varepsilon_{f}^{\mathrm{MC}} \left( \pT, \eta \right)},
  \label{eq:btagsf}
\end{equation}
where $f$ is the jet flavor, $\varepsilon_{f}^{\mathrm{data}} \left( \pT, \eta \right)$ is the flavor tagging efficiency in data, and $\varepsilon_{f}^{\mathrm{mc}} \left( \pT, \eta \right)$ is the flavor tagging efficiency in MC. One method of determining the tagging efficiency in data is by applying the tag-and-probe (TnP) technique to data events passing a selection for single-lepton \qrkt\qrktbar\ decays. Because top quarks decay into a \bosW\ and a bottom quark nearly 96\% of the time and the \bosW\ boson decays hadronically around 67\% of the time, such a selection of events where one of the \bosW bosons decays leptonically and the other hadronially offers at least two \qrkb-jets and multiple light jets. If a data event contains a \qrkb-jet which satisfies a tagging requirement, the remaining ``probe'' jets are used to provide an unbiased estimate of the \qrkb-tagging efficiency from the fraction of probe jets passing the tagging requirement with respect to all probe jets according to the definition
\begin{equation}
  \varepsilon_{b} = \frac{N_{b}^{\mathrm{tagged}}}{N_{b}^{\mathrm{vetoed}} + N_{b}{\mathrm{tagged}}},
  \label{eq:btageff}
\end{equation}
where $N_{b}^{\mathrm{tagged}}$ is the number of probe jets passing the tagging requirements and $N_{b}^{\mathrm{vetoed}}$ is the number of probe jets failing the tagging requirements.

The \qrkb-tagging scale factors incur systematic effects through the measurement of the \qrkb-tagging efficiency. These systematic sources can arise from theory and simulation, such as uncertainties for the factorization and renormalization scales, the branching fractions of \qrkb-hadron decays, and gluon splitting, or data, such as uncertainties for pileup and the jet energy scale and resolution. The \qrkb-tagging algorithms and their performance, as well as the determination of their efficiency scale factors and uncertainty, are described in detail in Ref. \cite{CMSBTAG}.

\section{Missing Transverse Energy}

Although the CMS detector has a near hermetic design, neutrinos and perhaps other theorized particles which are neutral and weakly interacting escape detection. Their presence is indicated by an imbalance in the transverse momentum sum of all ``visible'' particles as illustrated in Figure \ref{fig:metdiag}, from which it is inferred that the energy missing from the final state must have been carried away by ``invisible'' particles to satisfy energy conservation. This imbalance motivates the definition of missing transverse momentum $vec{p}_{\mathrm{T}}^{\mathrm{miss}}$ in an event as
\begin{equation}
  \vec{p}_{\mathrm{T}}^{\mathrm{miss}} = - \sum \vec{p}_{\mathrm{T}},
  \label{eq:rawmet}
\end{equation}
or the negative vectorial sum over the transverse momenta of all visible final state particles. The missing transverse energy \pTmiss\ is taken to be the magnitude of $\vec{p}_{\mathrm{T}}^{\mathrm{miss}}$. A complete treatment of the reconstruction of \pTmiss\, including different methods and their performance, may be found in Ref. \cite{CMSMET}, but the following discussion focuses on the standard approach.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=3.5in]{images/met_schematic}
    \caption[Illustration of Missing Transverse Momentum]{An illustration of missing transverse momentum \pTmiss (gray dashed line), which is inferred from the imbalanced transverse momentum sum of visible particles which were detected by their tracks (black solid lines) and calorimeter energy deposits (blue rectangles). The symbol $\vec{\slashed{E}}_{T}$ which used to denote missing transverse momentum has been deprecated.\cite{METDIAGRAM}}
    \label{fig:metdiag}
\end{figure}

The reconstruction of \pTmiss\ is challenging because it relies on the accurate reconstruction of all other particles, the precise measurements of their properties, and inherits all of their corrections and the effects of their systematic sources. Given its robust performance, the standard PF algorithm is applied towards the reconstruction of \pTmiss, where the vectorial transverse momentum sum is performed over all PF candidate particles, to produce PF \pTmiss. However, the quantity at this stage is considered ``raw'' \pTmiss\ because it is systematically different from true \pTmiss\ due to non-linear detector responses and misreconstruction of the PF candidates which take on the role of the visible particles. The quantity typically used is \textit{Type-I corrected} \pTmiss, which propagates the jet energy scale (JES) corrections to the determination of \pTmiss. By distinguishing those PF candidates which have been clustered together as jets, we can reformulate Equation \ref{eq:rawmet} to naturally incorporate the JES corrections
\begin{equation}
  \vec{p}_{\mathrm{T}}^{\mathrm{miss}} = - \sum_{\mathrm{jets}} \vec{p}_{\mathrm{T}}^{\mathrm{JEC}} - \sum_{\mathrm{unclustered}} \vec{p}_{\mathrm{T}},
  \label{eq:type1met}
\end{equation}
where $\vec{p}_{\mathrm{T}}^{\mathrm{JEC}}$ denotes the $\vec{p}_{\mathrm{T}}$ of the corrected jets.

In addition to these corrections, \textit{filters} are designed by the JetMET POG to help physics analyses reject events with artificially large \pTmiss\ due to specific noise sources. The following filters were recommended to be applied for 2017 data, as well as MC simulation under 2017 data-taking conditions:
\begin{itemize}
  \item The \textbf{GoodVerticesFilter} rejects events with large \pTmiss\ due to noisy vertex reconstruction caused by pileup. 
  \item The \textbf{GlobalTightHaloFilter} rejects events with large \pTmiss\ due to beam-halo particles, which are the result of beam protons interacting with the instrumentation used to collimate the beam or residual gas particles in the beam pipe vacuum. Information from the CSCs in the muon endcap are used to identify the passage of halo muons, which have a non-negligible chance of interacting with the calorimeters to produce clusters with energy on the order of several hundred \GeV.
  \item The \textbf{HBHENoiseFilter} rejects events with large \pTmiss\ due to sporadic HCAL noise from the hybrid photodiodes (HPD) and readout box (RBX) electronics and ion feedback noise that can affect several HPD pixels. These spurious signals can be identified by their high HPD occupancy and pulse shape infromation from the RBX. An additional \textbf{HBHENoiseIsoFilter} performs the same task but adds additional isolation requirements to pootential HCAL noise clusters by applying a topological filter that searches for neighboring activity in the HCAL as well as the ECAL and tracker.
  \item The \textbf{EcalDeadCellTriggerPrimitiveFilter} rejects events with large \pTmiss\ due to large energy losses from particles travelling through dead ECAL cells which have non-functioning data links.
  \item The \textbf{BadPFMuonFilter} rejects events with large \pTmiss\ due to the inclusion of poor quality muon candidates which barely passed the PF algorthim's thresholds and have large mismeasured \pT.
  \item The \textbf{BadChargedCandidateFilter} rejects events with large \pTmiss\ due to the misreconstruction of poor quality muon candidates, where the PF algorithm incorrectly associates them with an energetic HCAL cluster to form a charged hadron candidate.
  \item The \textbf{EcalBadCalibFilter} rejects events with large \pTmiss\ due to sporadic high energy noise found in endcap crystals in the very forward region of the ECAL.
  \item The \textbf{eeBadScFilter}, which is applicable for data and not MC simulation, rejects events with large \pTmiss\ due to high-amplitude anomalous pulses produced by data channels from the ECAL endcap which are identified by the total energy and number of low-quality hits within an ECAL supercluster.
\end{itemize}

